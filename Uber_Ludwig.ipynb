{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Uber_Ludwig.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/Uber-Ludwig/blob/master/Uber_Ludwig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "qYaQXAAYiFxz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/750/1*IrwMuRkv6s9_mZib4pMRCg.png)\n",
        "\n",
        "\n",
        "#Introduction\n",
        "\n",
        "Uber continues its spree of deep learning technology releases. Since last year, the Uber AI Labs team has open sourced different frameworks that enable many of the fundamental building blocks of deep learning solutions. In February of this year Uber AI labs came up with a deep learning toolbox named Ludwig. This is a toolbox for training deep learning models without writing any kind of code.\n",
        "\n",
        "Training is one of the most developer intensive aspects of deep learning applications. Typically, data scientists spend numerous hours experimenting with different deep learning models to better perform about a specific training datasets. This process involves more than just training including several other aspects such as model comparison, evaluation, workload distribution and many others. Simplifying the training processes is the number one factor that can streamline the experimentation phase in deep learning solutions.\n",
        "\n",
        "\n",
        "##What is Ludwig ?\n",
        "\n",
        "\n",
        "Ludwig is a TensorFlow based toolbox that allows to train and test deep learning models without the need to write code. Incubated at Uber for the last two years, Ludwig was finally open sourced to incorporate the contributions of the data science community. Conceptually, Ludwig was created under five fundamental principles:\n",
        "\n",
        "**1. No coding required**: no coding skills are required to train a model and use it for obtaining predictions.\n",
        "\n",
        "**2. Generality**: a new data type-based approach to deep learning model design that makes the tool usable across many different use cases.\n",
        "\n",
        "**3. Flexibility**: experienced users have extensive control over model building and training, while newcomers will find it easy to use.\n",
        "\n",
        "**4. Extensibility**: easy to add new model architecture and new feature data types.\n",
        "\n",
        "**5. Understandability**: deep learning model internals are often considered black boxes, but we provide standard visualizations to understand their performance and compare their predictions.\n",
        "\n",
        "\n",
        "\n",
        "This post contains the following:\n",
        "\n",
        "1. Ludwig model Architecture\n",
        "2. Injecting prior knowledge in the Architecture\n",
        "3. Summary of the Architecture\n",
        "4. Ludwig in Action\n",
        "5. Closing Summary\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5YFfjRIgkEWE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All you need to provide is a CSV file containing your data, a list of columns to use as inputs, and a list of columns to use as outputs, Ludwig will do the rest. Simple commands can be used to train models both locally and in a distributed way, and to use them to predict on new data.\n",
        "\n",
        "\n",
        "Ludwig allows its users to train a deep learning model by providing just a tabular file (like CSV) containing the data and a YAML configuration file that specifies which columns of the tabular file are input features and which are output target variables. The simplicity of the configuration file enables faster prototyping, potentially reducing hours of coding down to a few minutes. If more than one output target variable is specified, Ludwig will perform** multi-task learning**, learning to predict all the outputs simultaneously, a task that usually requires custom code.\n",
        "\n",
        "\n",
        "#The Ludwig model Architecture\n",
        "\n",
        "The main new idea that Ludwig introduces is the notion of data type-specific encoders and decoders, which results in a highly modularized and extensible architecture: each type of data supported (text, images, categories, and so on) has a specific preprocessing function. In short, encoders map the raw data to tensors, and decoders map tensors to the raw data.The model definition can contain additional information, in particular preprocessing information for each feature in the dataset, which encoder or decoder to use for each feature, architectural parameters for each encoder and decoder, and training parameters. Ludwig uses a type of model called the** Encoder- Combiner-Decoder model**. Encoder-Combiner-Decoder is a novel deep learning architecture that allows for heterogeneous input and output types and the** injection of prior knowledge** in the\n",
        "form of architecture choices.\n",
        "\n",
        "##Encoder -Combiner- Decoder model\n",
        "\n",
        "\n",
        "The combination of deep-and-wide with multi-task learning inspired losses forms the basis of a new general architecture , the Encoder-Combiner-Decoder (ECD), depicted in the figure below.\n",
        "\n",
        "\n",
        "##![alt text](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "### The Encoder part \n",
        "\n",
        "In the encoder part of the architecture, each single input feature is encoded by a sub-part of the model, depending on its type. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "More formally each encoder is a function:-\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##$_{et(ϕi (x))}(ϕi(x))$ \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "orBrlUQ4qc7S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "where  $ϕi(x)$  is the i-th input feature and  $t(ϕ(x))$ is the type of feature.\n",
        "\n",
        "\n",
        "\n",
        "For instance, text features can by encoded by a Character-Based\n",
        "CNN or by a Bidirectional RNN on the word sequence, categorical\n",
        "features can be encoded through a linear projection into an embedding space, binary feature can be encoded with one single number and numerical features can be encoded through a single neuron\n",
        "that acts as a learned scaling factor. Each of these different encoders outputs a vector encoding for the input feature they deal with.\n",
        "\n",
        "\n",
        "### The Combiner Part\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In the combiner part, those vectors are concatenated as they are in\n",
        "the wide-and-deep approach, but the concatenation is optionally\n",
        "followed by fully connected layers that can learn some non-linear\n",
        "combination of the representations obtained so far. The combiner\n",
        "is needed in any circumstance when there is more than one input\n",
        "feature, like in this case, but could be skipped if there’s more there\n",
        "is only one input feature(see the figure above). A combiner is defined as a function c(x)\n",
        "so that:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##$$c(x) = f ([_{et(ϕ_{0}(x))}(ϕ_{0}(x)), . . . ,_{et(ϕ_{n}(x))}(ϕ_{n}(x))])$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "where [...] is the concatenation operator, n is the number of input\n",
        "features, and f is a multi-layer perceptron.\n",
        "\n",
        "###The Decoder Part\n",
        "\n",
        "Finally, in the decoder part, different output features have different “heads”, with each of them predicting a different output feature depending on their type. Each output decoder can contain an arbitrary number of additional fully connected layers between the output of the combiner and the\n",
        "layer responsible for the prediction. This makes it possible to have a multi-task model with weights shared among all the tasks up to the combiner and have a set of weights that are task-specific for\n",
        "increased flexibility. Each decoder is a function d that returns a loss\n",
        "and is defined as:-\n",
        "\n",
        "\n",
        "##$$d_{ϕi (x)}(x) = f_{ϕi (x)}(c(x))$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GOIUdG65tzMS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "where*** f*** is again a multi layer perceptron and there could be different * fs * for different output features. \n",
        "\n",
        "###The Output Features\n",
        "\n",
        "Categorical output features\n",
        "are treated as a multi-class classification task: they use the output\n",
        "of the combiner and pass it through softmax layer, and return a\n",
        "categorical cross entropy loss. Numeric output features are treated\n",
        "as a regression task: they use the output of the combiner to predict\n",
        "a single value, and return a mean squared error loss. Binary output\n",
        "features are treated as a binary classification task: they use the output of the combiner as input for a layer with a activation, and return a cross entropy loss. The sum of all the losses coming from the different output features is optimized through any variant of stochastic gradient descent in multi-task learning fashion.\n",
        "\n",
        "##$$L(x) =\\sum_{i ∈o(x)}w_{i}d_{ϕi (x)}(x)$$\n",
        "\n",
        "\n",
        "where $o(x)$ is the set of indices of output features, and $w_{i}$\n",
        "is a user\n",
        "defined weight for the specific output feature."
      ]
    },
    {
      "metadata": {
        "id": "Rs-BPdNubbAT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The ECD architecture provides **three key benefits**. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   It directly incorporates all raw input features, eschewing the need for preprocessing outside of                 mapping textual and categorical features into integers, as well as learning a single model to predict         both contact types and reply templates.\n",
        "\n",
        "2.   Finally, the architecture enables an easy method for swapping and comparing different encoders.\n",
        "\n",
        "\n",
        "3. Each of them can be parametrized independently, specifying the number of layers, the size of convolutional filters of each layer and how many stacked RNNs to use."
      ]
    },
    {
      "metadata": {
        "id": "YfeGL2racjs7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Injecting Prior Knowlege into the Architecture\n",
        "\n",
        "\n",
        "The ECD architecture makes it possible to further improve model\n",
        "performance by injecting prior knowledge about each task directly\n",
        "at the model architecture level. This can bre done by, the following:-\n",
        "\n",
        "\n",
        " ### *** 1. Predicting paths in a tree***\n",
        " \n",
        " \n",
        "Using a tree and predicting a path in the tree that leads to the target output\n",
        "rather than predicting the output directly. Luckily for us, the architecture is already in a hierarchial format, so this makes it easy.We can put the entire Hierarchy in a tree and predict the target output.\n",
        "\n",
        "\n",
        "###*** 2. Adding decoder dependencies***\n",
        "\n",
        "\n",
        "As different decoders can have a set of weights that is specific to\n",
        "output feature they try to predict, the output of the decoder is injected as an additional input, concatenating it with the output of the combiner(see the figure above). More\n",
        "generally,  the ECD architecture allows for dependencies between\n",
        "output features. The dependencies must form a directed acyclic\n",
        "graph for the computational graph to be built as when building\n",
        "it the outputs of the decoders of all the output features that the\n",
        "current output feature is dependent on is concatenated with the\n",
        "output of the combiner.\n",
        "\n",
        "\n",
        "\n",
        "The hypothesis is that adding dependencies among output features would help in the overall performance in predicting output features with dependencies should improve due to to the prior knowledge injection.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LPRr3z_xDLiZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Summary of the Architecture\n",
        "\n",
        "\n",
        "Each data type may have more than one encoder and decoder. For instance, text can be encoded with a convolutional neural network (CNN), a recurrent neural network (RNN), or other encoders. The user can then specify which one to use and its hyperparameters directly in the model definition file without having to write a single line of code.\n",
        "\n",
        "This versatile and flexible encoder-decoder architecture makes it easy for less experienced deep learning practitioners to train models for diverse machine learning tasks, such as text classification, object classification, image captioning, sequence tagging, regression, language modeling, machine translation, time series forecasting, and question answering. This opens up a variety of use cases that would typically be out of reach for inexperienced practitioners, and allows users experienced in one domain to approach new domains\n"
      ]
    },
    {
      "metadata": {
        "id": "WQQaY1k1pA3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Ludwig in Action\n",
        "\n",
        "Data scientists will use Ludwig for two main functionalities: training and predictions. Suppose that we are working on a text classification scenario with the following dataset:-\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/750/1*UuS_PhoB9O7cm6VERzf8eg.png)\n",
        "\n",
        "\n",
        "We can get started with Ludwig by installing it using the following command:"
      ]
    },
    {
      "metadata": {
        "id": "C5iqCnwh5kki",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install ludwig\n",
        "python -m spacy download en\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2JgdWGSx5qjZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next step would be to configure a model definition YAML file that specifies the input and output features of the model"
      ]
    },
    {
      "metadata": {
        "id": "9Hu3Tn2L5vcw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "input_features:\n",
        "    -\n",
        "        name: text\n",
        "        type: text\n",
        "        encoder: parallel_cnn\n",
        "        level: word\n",
        "\n",
        "output_features:\n",
        "    -\n",
        "        name: class\n",
        "        type: category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3wLkhDG51XJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With those two inputs(training data and YAML configuration), we can train a deep learning model using the following command"
      ]
    },
    {
      "metadata": {
        "id": "FWALCSiF6FrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ludwig experiment \\\n",
        "  --data_csv reuters-allcats.csv \\\n",
        "  --model_definition_file model_definition.yaml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O0edWepk6L9A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ludwig provides a series of visualizations that can be used during training and predictions. For instance, the learning curve visualization give us an idea of the training and testing performance of the model. (Ludwig)\n",
        "\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/938/1*l8QRBL6MB1yiwjOmQ0dX1Q.png)\n",
        "\n",
        " These learning curves show loss and accuracy over training epochs.\n",
        "\n",
        "\n",
        "After training we can evaluate the predictions of the model using the following command:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "c8PqS3Um6kKR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ludwig predict --data_csv path/to/data.csv --model_path /path/to/model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9BtpUavy6lNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Other visualizations can be used to evaluate the performance of the model. (Source: Ludwig)\n",
        "\n",
        "\n",
        "![alt_text](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/image1-768x528.png)\n",
        "\n",
        "\n",
        "\n",
        "The complete Ludwig feature set is programmatically available via APIs. Recreating our example using Python is a matter of a few lines of code:\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mqJHCk8o7A8Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from ludwig import LudwigModel\n",
        "\n",
        "# train a model\n",
        "model_definition = {...}\n",
        "model = LudwigModel(model_definition)\n",
        "train_stats = model.train(training_dataframe)\n",
        "\n",
        "# or load a model\n",
        "model = LudwigModel.load(model_path)\n",
        "\n",
        "# obtain predictions\n",
        "predictions = model.predict(test_dataframe)\n",
        "\n",
        "model.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kkvji5EZ7D9A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Despite its robust capabilities, Ludwig provides a very extensible architecture for data scientists to incorporate their own encoders and decoders as well as functions to pre-process the data. For instance, creating a new Ludwig encoder is a matter of implementing the init and call methods as shown in the following code:\n"
      ]
    },
    {
      "metadata": {
        "id": "nYjU_AHm7HXB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def __init__(\n",
        "    self,\n",
        "    should_embed=True,\n",
        "    vocab=None,\n",
        "    representation='dense',\n",
        "    embedding_size=256,\n",
        "    embeddings_trainable=True,\n",
        "    pretrained_embeddings=None,\n",
        "    embeddings_on_cpu=False,\n",
        "    num_layers=1,\n",
        "    state_size=256,\n",
        "    cell_type='rnn',\n",
        "    bidirectional=False,\n",
        "    dropout=False,\n",
        "    initializer=None,\n",
        "    regularize=True,\n",
        "    reduce_output='last',\n",
        "    **kwargs\n",
        "):\n",
        "__call__(\n",
        "    self,\n",
        "    input_placeholder,\n",
        "    regularizer,\n",
        "    dropout,\n",
        "    is_training\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "szyc9qh57SoZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Closing Summary\n",
        "\n",
        "\n",
        "1. Ludwig is an incredibly helpful toolbox for the training and experimentation of deep learning models.\n",
        "\n",
        "2. Using Ludwig will allow even junior data scientists to train and test highly sophisticated deep learning     models without the need of writing any code.\n",
        "\n",
        "3. Ludwig provides a very extensible architecture for data scientists to incorporate their own encoders    and decoders as well as functions to pre-process the data.\n",
        "\n",
        "4. Ludwig’s simple training and interactive visualization processes can drastically shorten the experimentation cycles in deep learning applications allowing experts to focus on fine tuning the architecture of the target models instead of spending countless hours doing repetitive training work.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}